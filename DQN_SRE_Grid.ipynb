{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b279ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import learning_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b2c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_nxn:\n",
    "   \n",
    "    def __init__(self,n):\n",
    "        self.location = np.array([0,0])\n",
    "        self.goal = np.array([n-1,n-1])\n",
    "        self.n = n\n",
    "        \n",
    "    #Dynamics\n",
    "    # 0:up 1:right 2:down 3:left\n",
    "    def move(self,action):\n",
    "        if action == 0:\n",
    "            self.location[0] += 1\n",
    "            if self.location[0] == self.n:\n",
    "                self.location[0] = self.n-1\n",
    "            return self.location\n",
    "        if action == 1:\n",
    "            self.location[1] += 1\n",
    "            if self.location[1] == self.n:\n",
    "                self.location[1] = self.n-1\n",
    "            return self.location\n",
    "        if action == 2:\n",
    "            self.location[0] -= 1\n",
    "            if self.location[0] == -1:\n",
    "                self.location[0] = 0\n",
    "            return self.location\n",
    "        if action == 3:\n",
    "            self.location[1] -= 1\n",
    "            if self.location[1] == -1:\n",
    "                self.location[1] = 0\n",
    "            return self.location\n",
    "        \n",
    "    def checkReward(self):\n",
    "        if self.checkDone():\n",
    "            return +100\n",
    "        return -1\n",
    "    \n",
    "    def checkDone(self):\n",
    "        if (self.location == self.goal).all():\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def returnState(self):\n",
    "        return self.location\n",
    "    \n",
    "    def reverseAction(self,action):\n",
    "        if action == 0:\n",
    "            return 2\n",
    "        if action == 1:\n",
    "            return 3\n",
    "        if action == 2:\n",
    "            return 0\n",
    "        if action == 3:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204621ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006737104780279986\n",
      "yes1\n",
      "Episode 100\tAverage Score: -12.94\n",
      "Episode 200\tAverage Score: -15.00\n",
      "Episode 300\tAverage Score: -12.90\n",
      "Episode 400\tAverage Score: -12.97\n",
      "Episode 500\tAverage Score: -15.00\n",
      "Episode 600\tAverage Score: -10.79\n",
      "Episode 654\tScore: -15.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m agent \u001b[38;5;241m=\u001b[39m learning_agents\u001b[38;5;241m.\u001b[39mAgent_DQNHER(state_size\u001b[38;5;241m=\u001b[39mstate_shape,action_size \u001b[38;5;241m=\u001b[39m action_shape,seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m scores_her, terminal_ep_her \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_her\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 111\u001b[0m, in \u001b[0;36mdqn_her\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#Training (Refer to HER algorithm)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[0;32m--> 111\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m scores_window\u001b[38;5;241m.\u001b[39mappend(score)                       \u001b[38;5;66;03m# save most recent score\u001b[39;00m\n\u001b[1;32m    118\u001b[0m scores_window_printing\u001b[38;5;241m.\u001b[39mappend(score)              \u001b[38;5;66;03m# save most recent score\u001b[39;00m\n",
      "File \u001b[0;32m~/Research/cube_RL/learning_agents.py:350\u001b[0m, in \u001b[0;36mAgent_DQNHER.train_call\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_call\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m BATCH_SIZE:\n\u001b[0;32m--> 350\u001b[0m         experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(experiences, GAMMA)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;124;03m\"\"\" +Q TARGETS PRESENT \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Research/cube_RL/learning_agents.py:274\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m    273\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 274\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    275\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    276\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mnext_state \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Defining DQN-HER Algorithm\n",
    "import copy\n",
    "\n",
    "n = 5\n",
    "grid = grid_nxn(n)\n",
    "\n",
    "state_shape = grid.returnState().shape[0]\n",
    "action_shape = 4\n",
    "\n",
    "\n",
    "\n",
    "def dqn_her(n_episodes=100000, max_t=int(3*n), eps_start=1.0, eps_end=0.1, eps_decay=0.99995):\n",
    "    print(pow(eps_decay,n_episodes))\n",
    "\n",
    "    scores = []                 # list containing scores from each episode\n",
    "    scores_window_printing = deque(maxlen=10) # For printing in the graph\n",
    "    scores_window= deque(maxlen=100)  # last 100 scores for checking if the avg is more than 195\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    hindsight_goals = []\n",
    "   \n",
    "    #Check if agent learns to solve a cube that is one move away from goal state\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        grid = grid_nxn(n)\n",
    "        fin_goal = grid.goal\n",
    "        state = grid.returnState()\n",
    "        score = 0\n",
    "        done = False\n",
    "        h_flag = 0\n",
    "        \n",
    "        \n",
    "        traj_val = []\n",
    "         \n",
    "        # Run for one trajectory\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            #We store all we need for each trajectory\n",
    "            \n",
    "            #print(grid.returnState())\n",
    "            \n",
    "            #Choosing an action\n",
    "            action = agent.act(np.concatenate((state,fin_goal)), eps)\n",
    "\n",
    "            #Executing that action\n",
    "            grid.move(action)\n",
    "            \n",
    "            #Next state\n",
    "            next_state = grid.returnState()\n",
    "            \n",
    "            #Modified reward system\n",
    "            reward = grid.checkReward()\n",
    "        \n",
    "            \n",
    "            #Checking if the episode ended\n",
    "            if grid.checkDone():\n",
    "                done = True\n",
    "             \n",
    "            #print(state)\n",
    "            traj_val.append([state,action,reward,next_state,done])\n",
    "            #print(traj_val)\n",
    "            #agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        # Once the trajectory is done, append the final state that the trajectory reached to the state and push it to experience replay\n",
    "        psuedo_goal = next_state\n",
    "        \n",
    "        \n",
    "                \n",
    "        if (not(psuedo_goal == fin_goal).all()):\n",
    "            flag = 0\n",
    "        #if state != fin_goal:   \n",
    "            if len(hindsight_goals) == 0:\n",
    "                print('yes1')\n",
    "                hindsight_goals.append(psuedo_goal)\n",
    "            for hind_goal in hindsight_goals:\n",
    "                if ((state == hind_goal).all()):\n",
    "                    flag = 1\n",
    "            if flag == 0:\n",
    "                hindsight_goals.append(psuedo_goal)\n",
    "        \n",
    "        for sublist in traj_val:\n",
    "            new_state = np.concatenate((sublist[0],psuedo_goal))\n",
    "            new_next_state = np.concatenate((sublist[3],psuedo_goal))\n",
    "            agent.step(new_state, sublist[1], sublist[2], new_next_state, sublist[4])\n",
    "            \n",
    "        #Working on the hindsight learning\n",
    "        #print(len(hindsight_goals))\n",
    "        for hind_goal in hindsight_goals:\n",
    "            for sublist in traj_val:\n",
    "                #Altering the input state structure\n",
    "                new_state = np.concatenate((sublist[0],hind_goal))\n",
    "                \n",
    "                #Altering the reward\n",
    "                if ((sublist[3] == hind_goal).all):\n",
    "                    new_reward = 1 \n",
    "                    \n",
    "                #Altering the next state structure\n",
    "                new_next_state = np.concatenate((sublist[3],hind_goal))\n",
    "                agent.step(new_state, sublist[1], reward, new_next_state, sublist[4])\n",
    "                \n",
    "        \n",
    "        #Training (Refer to HER algorithm)\n",
    "        for _ in range(max_t):\n",
    "            agent.train_call()\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        scores_window.append(score)                       # save most recent score\n",
    "        scores_window_printing.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps)                 # decrease epsilon\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score), end=\"\")        \n",
    "        if i_episode % 100 == 0: \n",
    "            \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=100 - 2.5*n:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            break\n",
    "    return [np.array(scores),i_episode-100]\n",
    "\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = learning_agents.Agent_DQNHER(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "scores_her, terminal_ep_her = dqn_her()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db861a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining DQN-SRE Algorithm\n",
    "import copy\n",
    "\n",
    "n = 5\n",
    "grid = grid_nxn(n)\n",
    "\n",
    "state_shape = grid.returnState().shape[0]\n",
    "action_shape = 4\n",
    "\n",
    "\n",
    "\n",
    "def dqn_her(n_episodes=100000, max_t=int(3*n), eps_start=1.0, eps_end=0.1, eps_decay=0.99995):\n",
    "    print(pow(eps_decay,n_episodes))\n",
    "\n",
    "    scores = []                 # list containing scores from each episode\n",
    "    scores_window_printing = deque(maxlen=10) # For printing in the graph\n",
    "    scores_window= deque(maxlen=100)  # last 100 scores for checking if the avg is more than 195\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    hindsight_goals = []\n",
    "   \n",
    "    #Check if agent learns to solve a cube that is one move away from goal state\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        grid = grid_nxn(n)\n",
    "        fin_goal = grid.goal\n",
    "        state = grid.returnState()\n",
    "        score = 0\n",
    "        done = False\n",
    "        h_flag = 0\n",
    "        \n",
    "        \n",
    "        traj_val = []\n",
    "         \n",
    "        # Run for one trajectory\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            #We store all we need for each trajectory\n",
    "            \n",
    "            #print(grid.returnState())\n",
    "            \n",
    "            #Choosing an action\n",
    "            action = agent.act(np.concatenate((state,fin_goal)), eps)\n",
    "\n",
    "            #Executing that action\n",
    "            grid.move(action)\n",
    "            \n",
    "            #Next state\n",
    "            next_state = grid.returnState()\n",
    "            \n",
    "            #Modified reward system\n",
    "            reward = grid.checkReward()\n",
    "        \n",
    "            \n",
    "            #Checking if the episode ended\n",
    "            if grid.checkDone():\n",
    "                done = True\n",
    "             \n",
    "            #print(state)\n",
    "            traj_val.append([state,action,reward,next_state,done])\n",
    "            #print(traj_val)\n",
    "            #agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        # Once the trajectory is done, append the final state that the trajectory reached to the state and push it to experience replay\n",
    "        psuedo_goal = next_state\n",
    "        \n",
    "        \n",
    "                \n",
    "        if (not(psuedo_goal == fin_goal).all()):\n",
    "            flag = 0\n",
    "        #if state != fin_goal:   \n",
    "            if len(hindsight_goals) == 0:\n",
    "                print('yes1')\n",
    "                hindsight_goals.append(psuedo_goal)\n",
    "            for hind_goal in hindsight_goals:\n",
    "                if ((state == hind_goal).all()):\n",
    "                    flag = 1\n",
    "            if flag == 0:\n",
    "                hindsight_goals.append(psuedo_goal)\n",
    "        \n",
    "        for sublist in traj_val:\n",
    "            new_state = np.concatenate((sublist[0],psuedo_goal))\n",
    "            new_next_state = np.concatenate((sublist[3],psuedo_goal))\n",
    "            agent.step(new_state, sublist[1], sublist[2], new_next_state, sublist[4])\n",
    "            \n",
    "        #Working on the hindsight learning\n",
    "        #print(len(hindsight_goals))\n",
    "        for hind_goal in hindsight_goals:\n",
    "            for sublist in traj_val:\n",
    "                #Altering the input state structure\n",
    "                new_state = np.concatenate((sublist[0],hind_goal))\n",
    "                \n",
    "                #Altering the reward\n",
    "                if ((sublist[3] == hind_goal).all):\n",
    "                    new_reward = 1 \n",
    "                    \n",
    "                #Altering the next state structure\n",
    "                new_next_state = np.concatenate((sublist[3],hind_goal))\n",
    "                agent.step(new_state, sublist[1], reward, new_next_state, sublist[4])\n",
    "                \n",
    "        \n",
    "        #Training (Refer to HER algorithm)\n",
    "        for _ in range(max_t):\n",
    "            agent.train_call()\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        scores_window.append(score)                       # save most recent score\n",
    "        scores_window_printing.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps)                 # decrease epsilon\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score), end=\"\")        \n",
    "        if i_episode % 100 == 0: \n",
    "            \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=100 - 2.5*n:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            break\n",
    "    return [np.array(scores),i_episode-100]\n",
    "\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = learning_agents.Agent_DQNHER(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "scores_her, terminal_ep_her = dqn_her()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
